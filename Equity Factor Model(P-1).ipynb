{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de77a511-2156-4da0-b351-e97c67d811e5",
   "metadata": {},
   "source": [
    "Phase 1: Data Collection and Exploration for Equity Factor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4a5374-de59-43e8-896a-c8c376048e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching S&P 500 constituents from Wikipedia...\n",
      "Saved S&P 500 constituents to: data/sp500_constituents.csv\n",
      "Selected 80 stocks from 11 sectors\n",
      "\n",
      "Sector distribution:\n",
      "  Health Care: 10 stocks\n",
      "  Information Technology: 10 stocks\n",
      "  Utilities: 9 stocks\n",
      "  Industrials: 8 stocks\n",
      "  Financials: 7 stocks\n",
      "  Real Estate: 7 stocks\n",
      "  Consumer Staples: 7 stocks\n",
      "  Materials: 6 stocks\n",
      "  Consumer Discretionary: 6 stocks\n",
      "  Communication Services: 5 stocks\n",
      "  Energy: 5 stocks\n",
      "Fetching historical price data for 81 tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 81/81 [00:20<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving price data to cache: data/prices/prices_2020-01-01_to_2025-04-10_1d.pkl\n",
      "Fetching Fama-French factor data...\n",
      "Successfully added Fama-French factors\n",
      "Fetching market data for momentum and volatility factors...\n",
      "Successfully added momentum and volatility factors\n",
      "Saving factor data to cache: data/factors/factors_2020-01-01_to_2025-04-10.csv\n",
      "Custom OptionsDataFetcher not found, using mock implementation\n",
      "Fetching options data for 2 tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mock options data for SPY\n",
      "Saved options data for SPY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:01<00:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mock options data for TSLA\n",
      "Saved options data for TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving options data to cache: data/options/options_data_20250410.pkl\n",
      "Saved modeling dataset to: data/preprocessed/modeling_dataset.csv\n",
      "\n",
      "Data Collection Summary:\n",
      "Collected price data for 81 stocks\n",
      "Date range: 2020-01-02 00:00:00 to 2025-04-09 00:00:00\n",
      "Collected options data for 2 stocks\n",
      "Options tickers: SPY, TSLA\n",
      "\n",
      "=== Price Data Analysis ===\n",
      "Created price chart for HUBB\n",
      "Created price chart for MMM\n",
      "Created price chart for HII\n",
      "Created price chart for CTAS\n",
      "Created price chart for CARR\n",
      "\n",
      "=== Factor Data Analysis ===\n",
      "Created factor returns chart\n",
      "\n",
      "=== Options Data Analysis ===\n",
      "SPY options data:\n",
      "  Spot price: $352.72\n",
      "  Call options: 150\n",
      "  Put options: 150\n",
      "Created IV smile chart for SPY\n",
      "\n",
      "=== Modeling Data Analysis ===\n",
      "Modeling dataset shape: (106568, 17)\n",
      "Number of tickers: 81\n",
      "Created factor correlation matrix\n",
      "\n",
      "Exploratory data analysis complete. Charts saved to data/plots/ directory.\n",
      "\n",
      "Phase 1 (Data Collection and Exploration) complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import glob\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EquityDataCollector:\n",
    "    \"\"\"\n",
    "    Collects equity price data for multiple stocks and indexes\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir='data'):\n",
    "        \"\"\"\n",
    "        Initialize the data collector\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cache_dir : str\n",
    "            Directory to store cached data\n",
    "        \"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "            \n",
    "        # Create subdirectories\n",
    "        for subdir in ['prices', 'options', 'factors', 'preprocessed']:\n",
    "            subdir_path = os.path.join(cache_dir, subdir)\n",
    "            if not os.path.exists(subdir_path):\n",
    "                os.makedirs(subdir_path)\n",
    "        \n",
    "        # Initialize S&P 500 constituents\n",
    "        self.sp500 = None\n",
    "        \n",
    "    def get_sp500_constituents(self, refresh=False):\n",
    "        \"\"\"\n",
    "        Get current S&P 500 constituents with sector information\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        refresh : bool\n",
    "            Whether to refresh the constituents list from the web\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame: S&P 500 constituents with sector information\n",
    "        \"\"\"\n",
    "        cache_file = os.path.join(self.cache_dir, 'sp500_constituents.csv')\n",
    "        \n",
    "        if os.path.exists(cache_file) and not refresh:\n",
    "            # Load from cache\n",
    "            print(f\"Loading S&P 500 constituents from cache: {cache_file}\")\n",
    "            sp500 = pd.read_csv(cache_file)\n",
    "        else:\n",
    "            try:\n",
    "                # Scrape Wikipedia for S&P 500 constituents\n",
    "                print(\"Fetching S&P 500 constituents from Wikipedia...\")\n",
    "                url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "                tables = pd.read_html(url)\n",
    "                sp500 = tables[0]\n",
    "                \n",
    "                # Rename columns\n",
    "                sp500 = sp500.rename(columns={\n",
    "                    'Symbol': 'ticker',\n",
    "                    'Security': 'company_name',\n",
    "                    'GICS Sector': 'sector',\n",
    "                    'GICS Sub-Industry': 'sub_industry',\n",
    "                    'CIK': 'cik',\n",
    "                    'Headquarters Location': 'location',\n",
    "                    'Date added': 'date_added',\n",
    "                    'Founded': 'founded'\n",
    "                })\n",
    "                \n",
    "                # Clean tickers (some may have dots or special characters)\n",
    "                sp500['ticker'] = sp500['ticker'].str.replace('.', '-')\n",
    "                \n",
    "                # Save to cache\n",
    "                sp500.to_csv(cache_file, index=False)\n",
    "                print(f\"Saved S&P 500 constituents to: {cache_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching S&P 500 constituents: {e}\")\n",
    "                # Use a fallback if available\n",
    "                if os.path.exists(cache_file):\n",
    "                    print(f\"Using cached S&P 500 constituents as fallback\")\n",
    "                    sp500 = pd.read_csv(cache_file)\n",
    "                else:\n",
    "                    # Create a simple fallback with major stocks\n",
    "                    print(\"Creating fallback S&P 500 data with major stocks\")\n",
    "                    sp500 = pd.DataFrame({\n",
    "                        'ticker': ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA', 'JPM', 'V', 'PG',\n",
    "                                 'UNH', 'HD', 'BAC', 'XOM', 'PFE', 'CSCO', 'CVX', 'ADBE', 'CRM', 'NFLX'],\n",
    "                        'company_name': ['Apple Inc.', 'Microsoft Corp.', 'Amazon.com Inc.', 'Alphabet Inc.',\n",
    "                                       'Meta Platforms Inc.', 'Tesla Inc.', 'NVIDIA Corp.', 'JPMorgan Chase & Co.',\n",
    "                                       'Visa Inc.', 'Procter & Gamble Co.', 'UnitedHealth Group Inc.',\n",
    "                                       'Home Depot Inc.', 'Bank of America Corp.', 'Exxon Mobil Corp.',\n",
    "                                       'Pfizer Inc.', 'Cisco Systems Inc.', 'Chevron Corp.',\n",
    "                                       'Adobe Inc.', 'Salesforce Inc.', 'Netflix Inc.'],\n",
    "                        'sector': ['Information Technology', 'Information Technology', 'Consumer Discretionary',\n",
    "                                 'Communication Services', 'Communication Services', 'Consumer Discretionary',\n",
    "                                 'Information Technology', 'Financials', 'Financials', 'Consumer Staples',\n",
    "                                 'Health Care', 'Consumer Discretionary', 'Financials', 'Energy',\n",
    "                                 'Health Care', 'Information Technology', 'Energy', 'Information Technology',\n",
    "                                 'Information Technology', 'Communication Services']\n",
    "                    })\n",
    "                    sp500.to_csv(cache_file, index=False)\n",
    "        \n",
    "        self.sp500 = sp500\n",
    "        return sp500\n",
    "    \n",
    "    def get_sample_stocks(self, n_stocks=50, min_per_sector=3, seed=42):\n",
    "        \"\"\"\n",
    "        Get a balanced sample of stocks from the S&P 500\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_stocks : int\n",
    "            Number of stocks to sample\n",
    "        min_per_sector : int\n",
    "            Minimum number of stocks per sector\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame: Sampled stocks with sector information\n",
    "        \"\"\"\n",
    "        if self.sp500 is None:\n",
    "            self.get_sp500_constituents()\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Ensure we have at least min_per_sector stocks from each sector\n",
    "        sectors = self.sp500['sector'].unique()\n",
    "        \n",
    "        # Initialize sampled stocks\n",
    "        sampled_stocks = pd.DataFrame()\n",
    "        \n",
    "        # First, take min_per_sector stocks from each sector\n",
    "        for sector in sectors:\n",
    "            sector_stocks = self.sp500[self.sp500['sector'] == sector]\n",
    "            \n",
    "            # If sector has fewer stocks than min_per_sector, take all of them\n",
    "            if len(sector_stocks) <= min_per_sector:\n",
    "                sampled_stocks = pd.concat([sampled_stocks, sector_stocks])\n",
    "            else:\n",
    "                # Sample min_per_sector stocks from this sector\n",
    "                sector_sample = sector_stocks.sample(min_per_sector)\n",
    "                sampled_stocks = pd.concat([sampled_stocks, sector_sample])\n",
    "        \n",
    "        # If we need more stocks, sample randomly from the remaining\n",
    "        if len(sampled_stocks) < n_stocks:\n",
    "            remaining = self.sp500[~self.sp500['ticker'].isin(sampled_stocks['ticker'])]\n",
    "            additional_sample = remaining.sample(min(len(remaining), n_stocks - len(sampled_stocks)))\n",
    "            sampled_stocks = pd.concat([sampled_stocks, additional_sample])\n",
    "        \n",
    "        # If we have more stocks than requested, trim randomly\n",
    "        if len(sampled_stocks) > n_stocks:\n",
    "            sampled_stocks = sampled_stocks.sample(n_stocks)\n",
    "        \n",
    "        # Reset index\n",
    "        sampled_stocks = sampled_stocks.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Selected {len(sampled_stocks)} stocks from {len(sectors)} sectors\")\n",
    "        \n",
    "        # Print sector distribution\n",
    "        sector_counts = sampled_stocks['sector'].value_counts()\n",
    "        print(\"\\nSector distribution:\")\n",
    "        for sector, count in sector_counts.items():\n",
    "            print(f\"  {sector}: {count} stocks\")\n",
    "        \n",
    "        return sampled_stocks\n",
    "    \n",
    "    def fetch_historical_prices(self, tickers, start_date='2020-01-01', end_date=None, \n",
    "                              interval='1d', adjust=True, refresh=False):\n",
    "        \"\"\"\n",
    "        Fetch historical price data for multiple tickers\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        tickers : list\n",
    "            List of ticker symbols\n",
    "        start_date : str\n",
    "            Start date for data (YYYY-MM-DD)\n",
    "        end_date : str, optional\n",
    "            End date for data (YYYY-MM-DD), defaults to today\n",
    "        interval : str\n",
    "            Data interval ('1d', '1wk', '1mo')\n",
    "        adjust : bool\n",
    "            Whether to use adjusted prices\n",
    "        refresh : bool\n",
    "            Whether to refresh data from the source\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict: Dictionary of pandas.DataFrame price data by ticker\n",
    "        \"\"\"\n",
    "        # Set default end date to today if not provided\n",
    "        if end_date is None:\n",
    "            end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Create filename based on parameters\n",
    "        filename = f\"prices_{start_date}_to_{end_date}_{interval}.pkl\"\n",
    "        cache_file = os.path.join(self.cache_dir, 'prices', filename)\n",
    "        \n",
    "        # Check if data is cached and we don't want to refresh\n",
    "        if os.path.exists(cache_file) and not refresh:\n",
    "            print(f\"Loading cached price data from: {cache_file}\")\n",
    "            try:\n",
    "                price_data = pd.read_pickle(cache_file)\n",
    "                \n",
    "                # Check if we have all the tickers\n",
    "                missing_tickers = [ticker for ticker in tickers if ticker not in price_data]\n",
    "                \n",
    "                if not missing_tickers:\n",
    "                    return price_data\n",
    "                else:\n",
    "                    print(f\"Missing {len(missing_tickers)} tickers in cache. Fetching missing data...\")\n",
    "                    tickers = missing_tickers\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached price data: {e}\")\n",
    "                price_data = {}\n",
    "        else:\n",
    "            price_data = {}\n",
    "        \n",
    "        # Fetch data for all tickers\n",
    "        print(f\"Fetching historical price data for {len(tickers)} tickers...\")\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        for ticker in tqdm(tickers):\n",
    "            try:\n",
    "                # Fetch data with retry logic\n",
    "                attempts = 0\n",
    "                max_attempts = 3\n",
    "                success = False\n",
    "                \n",
    "                while not success and attempts < max_attempts:\n",
    "                    try:\n",
    "                        stock = yf.Ticker(ticker)\n",
    "                        hist = stock.history(start=start_date, end=end_date, interval=interval, auto_adjust=adjust)\n",
    "                        \n",
    "                        if not hist.empty:\n",
    "                            # Store in dictionary\n",
    "                            price_data[ticker] = hist\n",
    "                            success = True\n",
    "                        else:\n",
    "                            print(f\"No data returned for {ticker}, retrying ({attempts+1}/{max_attempts})...\")\n",
    "                            attempts += 1\n",
    "                            time.sleep(1)  # Wait before retrying\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "                        attempts += 1\n",
    "                        time.sleep(2)  # Longer wait on exception\n",
    "                \n",
    "                if not success:\n",
    "                    print(f\"Failed to fetch data for {ticker} after {max_attempts} attempts\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error with {ticker}: {e}\")\n",
    "        \n",
    "        # Save to cache\n",
    "        print(f\"Saving price data to cache: {cache_file}\")\n",
    "        pd.to_pickle(price_data, cache_file)\n",
    "        \n",
    "        return price_data\n",
    "    \n",
    "    def fetch_factor_data(self, start_date='2020-01-01', end_date=None, refresh=False):\n",
    "        \"\"\"\n",
    "        Fetch common risk factor data (Fama-French, momentum, volatility)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date : str\n",
    "            Start date for data (YYYY-MM-DD)\n",
    "        end_date : str, optional\n",
    "            End date for data (YYYY-MM-DD), defaults to today\n",
    "        refresh : bool\n",
    "            Whether to refresh data from the source\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame: Factor data\n",
    "        \"\"\"\n",
    "        # Set default end date to today if not provided\n",
    "        if end_date is None:\n",
    "            end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Create filename based on parameters\n",
    "        filename = f\"factors_{start_date}_to_{end_date}.csv\"\n",
    "        cache_file = os.path.join(self.cache_dir, 'factors', filename)\n",
    "        \n",
    "        # Check if data is cached and we don't want to refresh\n",
    "        if os.path.exists(cache_file) and not refresh:\n",
    "            print(f\"Loading cached factor data from: {cache_file}\")\n",
    "            try:\n",
    "                return pd.read_csv(cache_file, index_col=0, parse_dates=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached factor data: {e}\")\n",
    "                # Continue to fetch new data\n",
    "        \n",
    "        # Create a DataFrame with dates as index\n",
    "        start_dt = pd.to_datetime(start_date)\n",
    "        end_dt = pd.to_datetime(end_date)\n",
    "        \n",
    "        # Create date range\n",
    "        date_range = pd.date_range(start=start_dt, end=end_dt, freq='B')  # Business days\n",
    "        factor_data = pd.DataFrame(index=date_range)\n",
    "        \n",
    "        # First, try to get Fama-French 5 factors\n",
    "        try:\n",
    "            print(\"Fetching Fama-French factor data...\")\n",
    "            \n",
    "            # Define URL for Fama-French data\n",
    "            ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_daily_CSV.zip\"\n",
    "            \n",
    "            # Create a specific fallback function for Fama-French data\n",
    "            def get_fama_french_data():\n",
    "                # Direct download and read\n",
    "                try:\n",
    "                    raw_data = pd.read_csv(ff_url, skiprows=3)\n",
    "                    \n",
    "                    # Find where the data starts (after header rows)\n",
    "                    for i, row in raw_data.iterrows():\n",
    "                        if isinstance(row.iloc[0], str) and row.iloc[0].isdigit():\n",
    "                            data_start = i\n",
    "                            break\n",
    "                    else:\n",
    "                        data_start = 0\n",
    "                    \n",
    "                    # Extract actual data\n",
    "                    ff_data = raw_data.iloc[data_start:]\n",
    "                    \n",
    "                    # Convert first column to date if needed\n",
    "                    if ff_data.columns[0] != 'date':\n",
    "                        ff_data = ff_data.rename(columns={ff_data.columns[0]: 'date'})\n",
    "                    \n",
    "                    # Ensure date is in correct format\n",
    "                    ff_data['date'] = pd.to_datetime(ff_data['date'], format='%Y%m%d', errors='coerce')\n",
    "                    \n",
    "                    # Remove rows with invalid dates\n",
    "                    ff_data = ff_data.dropna(subset=['date'])\n",
    "                    \n",
    "                    # Set date as index\n",
    "                    ff_data = ff_data.set_index('date')\n",
    "                    \n",
    "                    # Convert numeric columns to float\n",
    "                    for col in ff_data.columns:\n",
    "                        ff_data[col] = pd.to_numeric(ff_data[col], errors='coerce')\n",
    "                    \n",
    "                    # Convert from percent to decimal\n",
    "                    for col in ff_data.columns:\n",
    "                        if ff_data[col].dtype == 'float64':\n",
    "                            ff_data[col] = ff_data[col] / 100.0\n",
    "                    \n",
    "                    return ff_data\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in Fama-French data extraction: {e}\")\n",
    "                    return None\n",
    "            \n",
    "            # Try to get Fama-French data\n",
    "            ff_factors = get_fama_french_data()\n",
    "            \n",
    "            # If successful, add to factor_data\n",
    "            if ff_factors is not None and not ff_factors.empty:\n",
    "                # Filter for date range\n",
    "                ff_factors = ff_factors[(ff_factors.index >= start_dt) & (ff_factors.index <= end_dt)]\n",
    "                \n",
    "                # Add to factor_data\n",
    "                for col in ff_factors.columns:\n",
    "                    if col in ['Mkt-RF', 'Mkt_RF', 'Mkt.RF', 'mkt_rf', 'Mkt RF']:\n",
    "                        factor_data['mkt_rf'] = ff_factors[col]\n",
    "                    elif col.lower() in ['smb']:\n",
    "                        factor_data['smb'] = ff_factors[col]\n",
    "                    elif col.lower() in ['hml']:\n",
    "                        factor_data['hml'] = ff_factors[col]\n",
    "                    elif col.lower() in ['rmw']:\n",
    "                        factor_data['rmw'] = ff_factors[col]\n",
    "                    elif col.lower() in ['cma']:\n",
    "                        factor_data['cma'] = ff_factors[col]\n",
    "                    elif col.lower() in ['rf']:\n",
    "                        factor_data['rf'] = ff_factors[col]\n",
    "                \n",
    "                print(\"Successfully added Fama-French factors\")\n",
    "            else:\n",
    "                print(\"Could not retrieve Fama-French factors, using simulated data\")\n",
    "                \n",
    "                # Create simulated Fama-French factors\n",
    "                np.random.seed(42)  # For reproducibility\n",
    "                \n",
    "                # Generate random values with realistic parameters\n",
    "                factor_data['mkt_rf'] = np.random.normal(0.0005, 0.01, len(date_range))  # Market minus risk-free\n",
    "                factor_data['smb'] = np.random.normal(0.0002, 0.005, len(date_range))    # Small minus big\n",
    "                factor_data['hml'] = np.random.normal(0.0001, 0.006, len(date_range))    # High minus low\n",
    "                factor_data['rmw'] = np.random.normal(0.0001, 0.004, len(date_range))    # Robust minus weak\n",
    "                factor_data['cma'] = np.random.normal(0.0001, 0.004, len(date_range))    # Conservative minus aggressive\n",
    "                factor_data['rf'] = np.random.normal(0.0001, 0.0001, len(date_range))    # Risk-free rate\n",
    "                \n",
    "                print(\"Using simulated Fama-French factors\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Fama-French factor data: {e}\")\n",
    "            \n",
    "            # Create simulated factors if not already present\n",
    "            if 'mkt_rf' not in factor_data.columns:\n",
    "                np.random.seed(42)  # For reproducibility\n",
    "                factor_data['mkt_rf'] = np.random.normal(0.0005, 0.01, len(date_range))  # Market minus risk-free\n",
    "                factor_data['smb'] = np.random.normal(0.0002, 0.005, len(date_range))    # Small minus big\n",
    "                factor_data['hml'] = np.random.normal(0.0001, 0.006, len(date_range))    # High minus low\n",
    "                factor_data['rmw'] = np.random.normal(0.0001, 0.004, len(date_range))    # Robust minus weak\n",
    "                factor_data['cma'] = np.random.normal(0.0001, 0.004, len(date_range))    # Conservative minus aggressive\n",
    "                factor_data['rf'] = np.random.normal(0.0001, 0.0001, len(date_range))    # Risk-free rate\n",
    "                \n",
    "                print(\"Using simulated Fama-French factors\")\n",
    "        \n",
    "        # Now get market data for momentum and volatility\n",
    "        try:\n",
    "            print(\"Fetching market data for momentum and volatility factors...\")\n",
    "            \n",
    "            # Use SPY as market proxy\n",
    "            spy = yf.download('SPY', start=start_date, end=end_date, progress=False)\n",
    "            \n",
    "            if not spy.empty and 'Close' in spy.columns:\n",
    "                # Calculate returns\n",
    "                spy['return'] = spy['Close'].pct_change()\n",
    "                \n",
    "                # Calculate momentum (12-month rolling return)\n",
    "                spy['momentum'] = spy['Close'].pct_change(252).fillna(0)\n",
    "                \n",
    "                # Calculate volatility (21-day rolling std dev of returns)\n",
    "                spy['volatility'] = spy['return'].rolling(21).std().fillna(spy['return'].std())\n",
    "                \n",
    "                # Create new DataFrame with momentum and volatility\n",
    "                spy_factors = pd.DataFrame({'momentum': spy['momentum'], 'volatility': spy['volatility']})\n",
    "                \n",
    "                # Align indices between factor_data and spy_factors using merge\n",
    "                # Convert spy index to DataFrame first\n",
    "                spy_df = spy_factors.reset_index()\n",
    "                if 'index' in spy_df.columns:\n",
    "                    spy_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "                elif 'Date' in spy_df.columns:\n",
    "                    spy_df.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                \n",
    "                # Convert factor_data index to DataFrame\n",
    "                factor_df = factor_data.reset_index()\n",
    "                if 'index' in factor_df.columns:\n",
    "                    factor_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "                \n",
    "                # Merge on date\n",
    "                merged_df = pd.merge(factor_df, spy_df, on='date', how='left')\n",
    "                \n",
    "                # Set date as index again\n",
    "                merged_df.set_index('date', inplace=True)\n",
    "                \n",
    "                # Update factor_data with the merged values\n",
    "                factor_data = merged_df\n",
    "                \n",
    "                print(\"Successfully added momentum and volatility factors\")\n",
    "            else:\n",
    "                print(\"Could not retrieve market data, using simulated momentum and volatility\")\n",
    "                \n",
    "                # Create simulated momentum and volatility\n",
    "                np.random.seed(43)  # Different seed from Fama-French\n",
    "                factor_data['momentum'] = np.random.normal(0.001, 0.02, len(date_range))\n",
    "                factor_data['volatility'] = np.abs(np.random.normal(0.01, 0.005, len(date_range)))\n",
    "                \n",
    "                print(\"Using simulated momentum and volatility factors\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching market data: {e}\")\n",
    "            \n",
    "            # Create simulated momentum and volatility if not already present\n",
    "            if 'momentum' not in factor_data.columns:\n",
    "                np.random.seed(43)  # Different seed from Fama-French\n",
    "                factor_data['momentum'] = np.random.normal(0.001, 0.02, len(date_range))\n",
    "                factor_data['volatility'] = np.abs(np.random.normal(0.01, 0.005, len(date_range)))\n",
    "                \n",
    "                print(\"Using simulated momentum and volatility factors\")\n",
    "        \n",
    "        # Fill missing values with forward and backward fill\n",
    "        factor_data = factor_data.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Save to cache\n",
    "        print(f\"Saving factor data to cache: {cache_file}\")\n",
    "        factor_data.to_csv(cache_file)\n",
    "        \n",
    "        return factor_data\n",
    "    \n",
    "    def get_options_data(self, tickers, dte_min=10, dte_max=120, refresh=False):\n",
    "        \"\"\"\n",
    "        Fetch options data for a list of tickers using an OptionsDataFetcher\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        tickers : list\n",
    "            List of ticker symbols\n",
    "        dte_min : int\n",
    "            Minimum days to expiration\n",
    "        dte_max : int\n",
    "            Maximum days to expiration\n",
    "        refresh : bool\n",
    "            Whether to refresh data from the source\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict: Dictionary of options data by ticker\n",
    "        \"\"\"\n",
    "        # Create filename based on parameters\n",
    "        today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "        filename = f\"options_data_{today}.pkl\"\n",
    "        cache_file = os.path.join(self.cache_dir, 'options', filename)\n",
    "        \n",
    "        # Check if data is cached and we don't want to refresh\n",
    "        if os.path.exists(cache_file) and not refresh:\n",
    "            print(f\"Loading cached options data from: {cache_file}\")\n",
    "            try:\n",
    "                options_data = pd.read_pickle(cache_file)\n",
    "                \n",
    "                # Check if we have all the tickers\n",
    "                missing_tickers = [ticker for ticker in tickers if ticker not in options_data]\n",
    "                \n",
    "                if not missing_tickers:\n",
    "                    return options_data\n",
    "                else:\n",
    "                    print(f\"Missing {len(missing_tickers)} tickers in cache. Fetching missing data...\")\n",
    "                    tickers = missing_tickers\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached options data: {e}\")\n",
    "                options_data = {}\n",
    "        else:\n",
    "            options_data = {}\n",
    "        \n",
    "        # Import your OptionsDataFetcher class\n",
    "        try:\n",
    "            # Try to import your existing module\n",
    "            from options_module import OptionsDataFetcher\n",
    "            options_fetcher = OptionsDataFetcher()\n",
    "            print(\"Using your custom OptionsDataFetcher\")\n",
    "        except ImportError:\n",
    "            # Create a basic mock OptionsDataFetcher if your module isn't available\n",
    "            print(\"Custom OptionsDataFetcher not found, using mock implementation\")\n",
    "            options_fetcher = MockOptionsDataFetcher()\n",
    "        \n",
    "        # Fetch options data for all tickers\n",
    "        print(f\"Fetching options data for {len(tickers)} tickers...\")\n",
    "        \n",
    "        for ticker in tqdm(tickers):\n",
    "            try:\n",
    "                # Fetch options data\n",
    "                option_chain = options_fetcher.get_option_chain(ticker, dte_min=dte_min, dte_max=dte_max)\n",
    "                \n",
    "                if option_chain is not None:\n",
    "                    options_data[ticker] = option_chain\n",
    "                    \n",
    "                    # Optional: Save individual ticker data as CSV\n",
    "                    calls_file = os.path.join(self.cache_dir, 'options', f\"{ticker}_calls.csv\")\n",
    "                    puts_file = os.path.join(self.cache_dir, 'options', f\"{ticker}_puts.csv\")\n",
    "                    \n",
    "                    option_chain['calls'].to_csv(calls_file, index=False)\n",
    "                    option_chain['puts'].to_csv(puts_file, index=False)\n",
    "                    \n",
    "                    print(f\"Saved options data for {ticker}\")\n",
    "                else:\n",
    "                    print(f\"No options data available for {ticker}\")\n",
    "                \n",
    "                # Be nice to the API\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching options data for {ticker}: {e}\")\n",
    "        \n",
    "        # Save to cache\n",
    "        print(f\"Saving options data to cache: {cache_file}\")\n",
    "        pd.to_pickle(options_data, cache_file)\n",
    "        \n",
    "        return options_data\n",
    "\n",
    "\n",
    "class MockOptionsDataFetcher:\n",
    "    \"\"\"Mock implementation of OptionsDataFetcher for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_date = datetime.datetime.now().date()\n",
    "    \n",
    "    def get_option_chain(self, ticker, dte_min=10, dte_max=120):\n",
    "        \"\"\"\n",
    "        Generate mock option chain data for testing\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ticker : str\n",
    "            Ticker symbol\n",
    "        dte_min : int\n",
    "            Minimum days to expiration\n",
    "        dte_max : int\n",
    "            Maximum days to expiration\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict: Mock option chain data with calls and puts\n",
    "        \"\"\"\n",
    "        print(f\"Generating mock options data for {ticker}\")\n",
    "        \n",
    "        # Create fake spot price - different for each ticker for variety\n",
    "        # Hash the ticker to get a consistent but varied price\n",
    "        ticker_hash = sum(ord(c) for c in ticker)\n",
    "        base_price = 100 + (ticker_hash % 400)  # Price between 100 and 500\n",
    "        spot_price = base_price + (np.random.random() * 10 - 5)  # Add some randomness\n",
    "        \n",
    "        # Generate expiration dates\n",
    "        current_date = self.current_date\n",
    "        expiration_dates = []\n",
    "        \n",
    "        # Weekly expirations\n",
    "        for week in range(1, 8):\n",
    "            dte = week * 7\n",
    "            if dte_min <= dte <= dte_max:\n",
    "                exp_date = (current_date + datetime.timedelta(days=dte)).strftime('%Y-%m-%d')\n",
    "                expiration_dates.append(exp_date)\n",
    "        \n",
    "        # Monthly expirations\n",
    "        for month in range(1, 6):\n",
    "            dte = month * 30\n",
    "            if dte_min <= dte <= dte_max:\n",
    "                exp_date = (current_date + datetime.timedelta(days=dte)).strftime('%Y-%m-%d')\n",
    "                expiration_dates.append(exp_date)\n",
    "        \n",
    "        # Generate strike prices (centered around spot price)\n",
    "        strike_range = 0.3  # Strike prices within ±30% of spot\n",
    "        strikes = np.linspace(\n",
    "            spot_price * (1 - strike_range),\n",
    "            spot_price * (1 + strike_range),\n",
    "            15\n",
    "        )\n",
    "        \n",
    "        # Generate option chains\n",
    "        calls_data = []\n",
    "        puts_data = []\n",
    "        \n",
    "        for exp_date in expiration_dates:\n",
    "            # Convert to datetime for calculations\n",
    "            exp_datetime = datetime.datetime.strptime(exp_date, '%Y-%m-%d').date()\n",
    "            dte = (exp_datetime - current_date).days\n",
    "            tte = dte / 365.0  # Time to expiration in years\n",
    "            \n",
    "            for strike in strikes:\n",
    "                # Calculate moneyness\n",
    "                moneyness = strike / spot_price\n",
    "                \n",
    "                # Base IV with smile effect\n",
    "                base_iv = 0.2 + 0.1 * (moneyness - 1) ** 2\n",
    "                \n",
    "                # Add term structure effect (higher IV for longer dates)\n",
    "                iv_term = 0.05 * np.sqrt(tte)\n",
    "                \n",
    "                # Generate call option\n",
    "                call = {\n",
    "                    'contractSymbol': f\"{ticker}{exp_date.replace('-', '')}C{int(strike * 100):08d}\",\n",
    "                    'strike': strike,\n",
    "                    'lastPrice': max(0.01, spot_price - strike) + np.random.random() * 2,\n",
    "                    'bid': max(0.01, spot_price - strike) * 0.95,\n",
    "                    'ask': max(0.01, spot_price - strike) * 1.05 + 0.1,\n",
    "                    'impliedVolatility': base_iv + iv_term,\n",
    "                    'volume': int(np.random.exponential(100) * (1.5 - abs(moneyness - 1))),\n",
    "                    'openInterest': int(np.random.exponential(500) * (1.5 - abs(moneyness - 1))),\n",
    "                    'inTheMoney': strike < spot_price,\n",
    "                    'expirationDate': exp_date,\n",
    "                    'dte': dte,\n",
    "                    'tte': tte,\n",
    "                    'moneyness': moneyness\n",
    "                }\n",
    "                \n",
    "                # Generate put option\n",
    "                put = {\n",
    "                    'contractSymbol': f\"{ticker}{exp_date.replace('-', '')}P{int(strike * 100):08d}\",\n",
    "                    'strike': strike,\n",
    "                    'lastPrice': max(0.01, strike - spot_price) + np.random.random() * 2,\n",
    "                    'bid': max(0.01, strike - spot_price) * 0.95,\n",
    "                    'ask': max(0.01, strike - spot_price) * 1.05 + 0.1,\n",
    "                    'impliedVolatility': base_iv + iv_term + 0.02,  # Puts typically have higher IV\n",
    "                    'volume': int(np.random.exponential(100) * (1.5 - abs(moneyness - 1))),\n",
    "                    'openInterest': int(np.random.exponential(500) * (1.5 - abs(moneyness - 1))),\n",
    "                    'inTheMoney': strike > spot_price,\n",
    "                    'expirationDate': exp_date,\n",
    "                    'dte': dte,\n",
    "                    'tte': tte,\n",
    "                    'moneyness': moneyness\n",
    "                }\n",
    "                \n",
    "                # Adjust prices for realistic values\n",
    "                call['lastPrice'] = max(0.01, call['lastPrice'])\n",
    "                put['lastPrice'] = max(0.01, put['lastPrice'])\n",
    "                call['bid'] = max(0.01, call['bid'])\n",
    "                put['bid'] = max(0.01, put['bid'])\n",
    "                call['ask'] = max(call['bid'] + 0.01, call['ask'])\n",
    "                put['ask'] = max(put['bid'] + 0.01, put['ask'])\n",
    "                \n",
    "                # Add a usePrice column (mid price)\n",
    "                call['usePrice'] = (call['bid'] + call['ask']) / 2\n",
    "                put['usePrice'] = (put['bid'] + put['ask']) / 2\n",
    "                \n",
    "                calls_data.append(call)\n",
    "                puts_data.append(put)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        calls_df = pd.DataFrame(calls_data)\n",
    "        puts_df = pd.DataFrame(puts_data)\n",
    "        \n",
    "        # Return options data\n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'spot_price': spot_price,\n",
    "            'calls': calls_df,\n",
    "            'puts': puts_df,\n",
    "            'quote_date': current_date,\n",
    "            'risk_free_rate': 0.05  # 5% risk-free rate\n",
    "        }\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses and cleans financial data for modeling\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir='data'):\n",
    "        \"\"\"\n",
    "        Initialize the data preprocessor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cache_dir : str\n",
    "            Directory for cached data\n",
    "        \"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "    def calculate_returns(self, price_data, period='daily'):\n",
    "        \"\"\"\n",
    "        Calculate returns from price data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        price_data : dict\n",
    "            Dictionary of price DataFrames by ticker\n",
    "        period : str\n",
    "            'daily', 'weekly', or 'monthly'\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict: Dictionary of return DataFrames by ticker\n",
    "        \"\"\"\n",
    "        returns_data = {}\n",
    "        \n",
    "        for ticker, prices in price_data.items():\n",
    "            try:\n",
    "                # Make a copy of the price data\n",
    "                df = prices.copy()\n",
    "                \n",
    "                # Calculate returns based on specified period\n",
    "                if 'Close' in df.columns:\n",
    "                    if period == 'daily':\n",
    "                        df['return'] = df['Close'].pct_change()\n",
    "                    elif period == 'weekly':\n",
    "                        df['return'] = df['Close'].pct_change(5)\n",
    "                    elif period == 'monthly':\n",
    "                        df['return'] = df['Close'].pct_change(21)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid period: {period}\")\n",
    "                else:\n",
    "                    # If no Close column, try Adj Close or use first numeric column\n",
    "                    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "                    if 'Adj Close' in df.columns:\n",
    "                        df['return'] = df['Adj Close'].pct_change()\n",
    "                    elif len(numeric_cols) > 0:\n",
    "                        df['return'] = df[numeric_cols[0]].pct_change()\n",
    "                    else:\n",
    "                        print(f\"Could not calculate returns for {ticker}: no suitable price column found\")\n",
    "                        df['return'] = 0  # Default to zero returns\n",
    "                \n",
    "                # Store in dictionary\n",
    "                returns_data[ticker] = df\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating returns for {ticker}: {e}\")\n",
    "                # Add placeholder with zero returns\n",
    "                returns_data[ticker] = prices.copy()\n",
    "                returns_data[ticker]['return'] = 0\n",
    "        \n",
    "        return returns_data\n",
    "    \n",
    "    def calculate_stock_factors(self, price_data, market_ticker='SPY'):\n",
    "        \"\"\"\n",
    "        Calculate stock-specific factors\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        price_data : dict\n",
    "            Dictionary of price DataFrames by ticker\n",
    "        market_ticker : str\n",
    "            Ticker to use as market benchmark\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame: Stock factors by ticker and date\n",
    "        \"\"\"\n",
    "        # Get market data if available, otherwise create dummy\n",
    "        if market_ticker in price_data:\n",
    "            market_data = price_data[market_ticker].copy()\n",
    "            \n",
    "            # Calculate market returns\n",
    "            if 'Close' in market_data.columns:\n",
    "                market_data['market_return'] = market_data['Close'].pct_change()\n",
    "            elif 'Adj Close' in market_data.columns:\n",
    "                market_data['market_return'] = market_data['Adj Close'].pct_change()\n",
    "            else:\n",
    "                # Use first numeric column\n",
    "                numeric_cols = market_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "                if len(numeric_cols) > 0:\n",
    "                    market_data['market_return'] = market_data[numeric_cols[0]].pct_change()\n",
    "                else:\n",
    "                    print(f\"Could not calculate market returns: no suitable price column found\")\n",
    "                    # Create a synthetic market return\n",
    "                    market_data['market_return'] = np.random.normal(0.0005, 0.01, len(market_data))\n",
    "        else:\n",
    "            print(f\"Market ticker {market_ticker} not found in price data. Using synthetic market returns.\")\n",
    "            \n",
    "            # Create a synthetic market index with dates from the first ticker\n",
    "            if price_data:\n",
    "                first_ticker = list(price_data.keys())[0]\n",
    "                market_data = pd.DataFrame(index=price_data[first_ticker].index)\n",
    "                market_data['market_return'] = np.random.normal(0.0005, 0.01, len(market_data))\n",
    "            else:\n",
    "                print(\"No price data available. Cannot calculate stock factors.\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Initialize factors list\n",
    "        factors = []\n",
    "        \n",
    "        # Process each stock\n",
    "        for ticker, prices in price_data.items():\n",
    "            if ticker == market_ticker:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Make a copy\n",
    "                df = prices.copy()\n",
    "                \n",
    "                # Calculate daily returns\n",
    "                if 'Close' in df.columns:\n",
    "                    df['return'] = df['Close'].pct_change()\n",
    "                elif 'Adj Close' in df.columns:\n",
    "                    df['return'] = df['Adj Close'].pct_change()\n",
    "                else:\n",
    "                    # Use first numeric column\n",
    "                    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "                    if len(numeric_cols) > 0:\n",
    "                        df['return'] = df[numeric_cols[0]].pct_change()\n",
    "                    else:\n",
    "                        print(f\"Could not calculate returns for {ticker}: no suitable price column found\")\n",
    "                        df['return'] = 0  # Default to zero returns\n",
    "                \n",
    "                # Calculate size factor (log of market cap if volume is available)\n",
    "                if 'Close' in df.columns and 'Volume' in df.columns:\n",
    "                    df['size'] = np.log(df['Close'] * df['Volume'])\n",
    "                else:\n",
    "                    # Use a random size factor if volume not available\n",
    "                    df['size'] = np.random.normal(10, 2, len(df))\n",
    "                \n",
    "                # Calculate value factor (approximate with price ratio)\n",
    "                if 'Low' in df.columns and 'High' in df.columns:\n",
    "                    df['value'] = df['Low'] / df['High'].rolling(20).mean()\n",
    "                else:\n",
    "                    # Use a random value factor\n",
    "                    df['value'] = np.random.normal(0.8, 0.1, len(df))\n",
    "                \n",
    "                # Calculate momentum (12-month rolling return)\n",
    "                if 'Close' in df.columns:\n",
    "                    df['momentum_12m'] = df['Close'].pct_change(252)\n",
    "                elif 'Adj Close' in df.columns:\n",
    "                    df['momentum_12m'] = df['Adj Close'].pct_change(252)\n",
    "                else:\n",
    "                    # Use cumulative return of daily returns\n",
    "                    df['momentum_12m'] = df['return'].rolling(252).apply(lambda x: (1 + x).prod() - 1)\n",
    "                \n",
    "                # Calculate volatility (1-month rolling volatility)\n",
    "                df['volatility'] = df['return'].rolling(21).std()\n",
    "                \n",
    "                # Calculate beta using a rolling window\n",
    "                # Align market returns with stock returns\n",
    "                if 'market_return' in market_data.columns:\n",
    "                    market_returns = market_data['market_return']\n",
    "                    aligned_market = market_returns.loc[df.index.intersection(market_returns.index)]\n",
    "                    \n",
    "                    # Ensure alignment\n",
    "                    if not aligned_market.empty and len(aligned_market) > 30:\n",
    "                        # Calculate betas using 1-year rolling window\n",
    "                        betas = []\n",
    "                        for i in range(252, len(df)):\n",
    "                            if i >= len(aligned_market):\n",
    "                                continue\n",
    "                            \n",
    "                            stock_window = df['return'].iloc[i-252:i]\n",
    "                            market_window = aligned_market.iloc[i-252:i]\n",
    "                            \n",
    "                            # Skip if either window has all NaNs\n",
    "                            if stock_window.isna().all() or market_window.isna().all():\n",
    "                                betas.append(np.nan)\n",
    "                                continue\n",
    "                            \n",
    "                            # Calculate covariance\n",
    "                            cov = np.cov(stock_window.fillna(0), market_window.fillna(0))[0, 1]\n",
    "                            var = np.var(market_window.fillna(0))\n",
    "                            \n",
    "                            # Calculate beta\n",
    "                            if var != 0:\n",
    "                                beta = cov / var\n",
    "                            else:\n",
    "                                beta = np.nan\n",
    "                            \n",
    "                            betas.append(beta)\n",
    "                        \n",
    "                        # Pad beginning with NaNs\n",
    "                        betas = [np.nan] * 252 + betas\n",
    "                        \n",
    "                        # Ensure length matches df\n",
    "                        betas = betas[:len(df)]\n",
    "                        \n",
    "                        # Assign to dataframe\n",
    "                        df['beta'] = betas\n",
    "                    else:\n",
    "                        # If not enough data, assign NaNs\n",
    "                        df['beta'] = np.nan\n",
    "                else:\n",
    "                    # If market returns not available, assign NaNs\n",
    "                    df['beta'] = np.nan\n",
    "                \n",
    "                # Fill NaNs\n",
    "                df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "                \n",
    "                # Prepare data for the factors DataFrame\n",
    "                stock_factors = df[['return', 'size', 'value', 'momentum_12m', 'volatility', 'beta']].copy()\n",
    "                stock_factors['ticker'] = ticker\n",
    "                \n",
    "                # Add to list\n",
    "                factors.append(stock_factors)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating factors for {ticker}: {e}\")\n",
    "                # Skip this ticker\n",
    "        \n",
    "        # Combine all factors\n",
    "        if factors:\n",
    "            combined_factors = pd.concat(factors)\n",
    "            \n",
    "            # Reset index to include date as a column\n",
    "            combined_factors = combined_factors.reset_index()\n",
    "            \n",
    "            # Rename index to date if needed\n",
    "            if 'index' in combined_factors.columns and 'date' not in combined_factors.columns:\n",
    "                combined_factors = combined_factors.rename(columns={'index': 'date'})\n",
    "            \n",
    "            return combined_factors\n",
    "        else:\n",
    "            print(\"No valid factors calculated.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def align_data(self, price_returns, factor_data):\n",
    "        \"\"\"\n",
    "        Align price returns with factor data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        price_returns : dict\n",
    "            Dictionary of return DataFrames by ticker\n",
    "        factor_data : pandas.DataFrame\n",
    "            Factor data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame: Aligned returns and factors\n",
    "        \"\"\"\n",
    "        # Initialize list to store aligned data\n",
    "        aligned_data = []\n",
    "        \n",
    "        # Process each ticker\n",
    "        for ticker, returns in price_returns.items():\n",
    "            try:\n",
    "                # Make a copy and ensure it has a return column\n",
    "                df = returns.copy()\n",
    "                if 'return' not in df.columns:\n",
    "                    print(f\"No return column for {ticker}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Keep only the return column and reset index\n",
    "                df_returns = df[['return']].copy()\n",
    "                df_returns = df_returns.reset_index()\n",
    "                df_returns['ticker'] = ticker\n",
    "                \n",
    "                # Ensure it has a date column - handle both 'Date' and 'index' cases\n",
    "                if 'date' not in df_returns.columns:\n",
    "                    if 'Date' in df_returns.columns:\n",
    "                        df_returns.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                    elif 'index' in df_returns.columns:\n",
    "                        df_returns.rename(columns={'index': 'date'}, inplace=True)\n",
    "                \n",
    "                # Add to list\n",
    "                aligned_data.append(df_returns)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error aligning data for {ticker}: {e}\")\n",
    "        \n",
    "        # Combine all returns if we have any\n",
    "        if not aligned_data:\n",
    "            print(\"No valid return data to align with factors.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        combined_returns = pd.concat(aligned_data)\n",
    "        \n",
    "        # Reset factor_data index to create date column\n",
    "        factor_data_reset = factor_data.reset_index()\n",
    "        \n",
    "        # Ensure it has a date column - handle both 'Date' and 'index' cases\n",
    "        if 'date' not in factor_data_reset.columns:\n",
    "            if 'Date' in factor_data_reset.columns:\n",
    "                factor_data_reset.rename(columns={'Date': 'date'}, inplace=True)\n",
    "            elif 'index' in factor_data_reset.columns:\n",
    "                factor_data_reset.rename(columns={'index': 'date'}, inplace=True)\n",
    "        \n",
    "        # Make sure date columns are datetime\n",
    "        combined_returns['date'] = pd.to_datetime(combined_returns['date'])\n",
    "        factor_data_reset['date'] = pd.to_datetime(factor_data_reset['date'])\n",
    "        \n",
    "        # Handle timezone differences - normalize datetime to remove timezone information\n",
    "        combined_returns['date'] = combined_returns['date'].dt.tz_localize(None)\n",
    "        factor_data_reset['date'] = factor_data_reset['date'].dt.tz_localize(None)\n",
    "        \n",
    "        # Perform the merge\n",
    "        try:\n",
    "            aligned = pd.merge(combined_returns, factor_data_reset, on='date', how='left')\n",
    "            \n",
    "            # Fill missing values\n",
    "            aligned = aligned.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            \n",
    "            return aligned\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error merging returns and factors: {e}\")\n",
    "            \n",
    "            # Create a minimal aligned dataset with returns only\n",
    "            print(\"Creating minimal aligned dataset with returns only.\")\n",
    "            aligned_data = pd.DataFrame()\n",
    "            \n",
    "            for ticker, returns in price_returns.items():\n",
    "                # Extract returns and add ticker\n",
    "                ticker_returns = returns[['return']].copy() if 'return' in returns.columns else pd.DataFrame(index=returns.index)\n",
    "                ticker_returns['ticker'] = ticker\n",
    "                \n",
    "                # Reset index to convert to column\n",
    "                ticker_returns = ticker_returns.reset_index()\n",
    "                \n",
    "                # Ensure it has a date column - handle both 'Date' and 'index' cases\n",
    "                if 'date' not in ticker_returns.columns:\n",
    "                    if 'Date' in ticker_returns.columns:\n",
    "                        ticker_returns.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                    elif 'index' in ticker_returns.columns:\n",
    "                        ticker_returns.rename(columns={'index': 'date'}, inplace=True)\n",
    "                \n",
    "                # Normalize timezone\n",
    "                ticker_returns['date'] = pd.to_datetime(ticker_returns['date'])\n",
    "                if hasattr(ticker_returns['date'].dt, 'tz_localize'):\n",
    "                    ticker_returns['date'] = ticker_returns['date'].dt.tz_localize(None)\n",
    "                \n",
    "                # Add to aligned data\n",
    "                if aligned_data.empty:\n",
    "                    aligned_data = ticker_returns\n",
    "                else:\n",
    "                    aligned_data = pd.concat([aligned_data, ticker_returns])\n",
    "            \n",
    "            # Add placeholder factor columns\n",
    "            for col in ['mkt_rf', 'smb', 'hml', 'momentum', 'volatility']:\n",
    "                aligned_data[col] = 0\n",
    "            \n",
    "            return aligned_data\n",
    "    \n",
    "    def prepare_modeling_dataset(self, price_data, factor_data, stock_factors=None):\n",
    "        \"\"\"\n",
    "        Prepare the final dataset for modeling\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        price_data : dict\n",
    "            Dictionary of price DataFrames by ticker\n",
    "        factor_data : pandas.DataFrame\n",
    "            Factor data\n",
    "        stock_factors : pandas.DataFrame, optional\n",
    "            Stock-specific factors\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame: Final modeling dataset\n",
    "        \"\"\"\n",
    "        # Calculate returns\n",
    "        returns_data = self.calculate_returns(price_data)\n",
    "        \n",
    "        # If stock factors not provided, calculate them\n",
    "        if stock_factors is None:\n",
    "            try:\n",
    "                stock_factors = self.calculate_stock_factors(price_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating stock factors: {e}\")\n",
    "                stock_factors = None\n",
    "        \n",
    "        # Align returns and factor data\n",
    "        try:\n",
    "            aligned_data = self.align_data(returns_data, factor_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error aligning returns and factors: {e}\")\n",
    "            \n",
    "            # Create a minimal dataset\n",
    "            aligned_data = pd.DataFrame()\n",
    "            \n",
    "            for ticker, returns in returns_data.items():\n",
    "                # Extract returns and add ticker\n",
    "                ticker_returns = returns[['return']].copy() if 'return' in returns.columns else pd.DataFrame(index=returns.index)\n",
    "                ticker_returns['ticker'] = ticker\n",
    "                \n",
    "                # Reset index to convert to column\n",
    "                ticker_returns = ticker_returns.reset_index()\n",
    "                \n",
    "                # Ensure it has a date column - handle both 'Date' and 'index' cases\n",
    "                if 'date' not in ticker_returns.columns:\n",
    "                    if 'Date' in ticker_returns.columns:\n",
    "                        ticker_returns.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                    elif 'index' in ticker_returns.columns:\n",
    "                        ticker_returns.rename(columns={'index': 'date'}, inplace=True)\n",
    "                \n",
    "                # Normalize timezone\n",
    "                ticker_returns['date'] = pd.to_datetime(ticker_returns['date'])\n",
    "                if hasattr(ticker_returns['date'].dt, 'tz_localize'):\n",
    "                    ticker_returns['date'] = ticker_returns['date'].dt.tz_localize(None)\n",
    "                \n",
    "                # Add to aligned data\n",
    "                if aligned_data.empty:\n",
    "                    aligned_data = ticker_returns\n",
    "                else:\n",
    "                    aligned_data = pd.concat([aligned_data, ticker_returns])\n",
    "            \n",
    "            # Add placeholder factor columns\n",
    "            for col in ['mkt_rf', 'smb', 'hml', 'momentum', 'volatility']:\n",
    "                aligned_data[col] = 0\n",
    "        \n",
    "        # Merge with stock factors if available\n",
    "        if stock_factors is not None and not stock_factors.empty:\n",
    "            try:\n",
    "                # Ensure stock_factors has date and ticker columns\n",
    "                stock_factors_reset = stock_factors.copy()\n",
    "                \n",
    "                # Handle date column in stock_factors\n",
    "                if 'date' not in stock_factors_reset.columns:\n",
    "                    if stock_factors_reset.index.name == 'date':\n",
    "                        stock_factors_reset = stock_factors_reset.reset_index()\n",
    "                    elif 'Date' in stock_factors_reset.columns:\n",
    "                        stock_factors_reset.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                    elif stock_factors_reset.index.name == 'Date':\n",
    "                        stock_factors_reset = stock_factors_reset.reset_index()\n",
    "                        stock_factors_reset.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                \n",
    "                # Convert date column to datetime if needed and normalize timezone\n",
    "                if 'date' in stock_factors_reset.columns:\n",
    "                    stock_factors_reset['date'] = pd.to_datetime(stock_factors_reset['date'])\n",
    "                    if hasattr(stock_factors_reset['date'].dt, 'tz_localize'):\n",
    "                        stock_factors_reset['date'] = stock_factors_reset['date'].dt.tz_localize(None)\n",
    "                \n",
    "                # Convert ticker column to string if needed\n",
    "                if 'ticker' in stock_factors_reset.columns:\n",
    "                    stock_factors_reset['ticker'] = stock_factors_reset['ticker'].astype(str)\n",
    "                \n",
    "                # Ensure aligned_data has date and ticker columns\n",
    "                if 'date' in aligned_data.columns:\n",
    "                    aligned_data['date'] = pd.to_datetime(aligned_data['date'])\n",
    "                    if hasattr(aligned_data['date'].dt, 'tz_localize'):\n",
    "                        aligned_data['date'] = aligned_data['date'].dt.tz_localize(None)\n",
    "                \n",
    "                if 'ticker' in aligned_data.columns:\n",
    "                    aligned_data['ticker'] = aligned_data['ticker'].astype(str)\n",
    "                \n",
    "                # Perform the merge\n",
    "                merge_columns = []\n",
    "                if 'date' in stock_factors_reset.columns and 'date' in aligned_data.columns:\n",
    "                    merge_columns.append('date')\n",
    "                if 'ticker' in stock_factors_reset.columns and 'ticker' in aligned_data.columns:\n",
    "                    merge_columns.append('ticker')\n",
    "                \n",
    "                # Check that we have at least one column to merge on\n",
    "                if merge_columns:\n",
    "                    final_data = pd.merge(aligned_data, stock_factors_reset, \n",
    "                                          on=merge_columns, how='left', suffixes=('', '_stock'))\n",
    "                else:\n",
    "                    print(\"No common columns found for merging. Using aligned data only.\")\n",
    "                    final_data = aligned_data\n",
    "            except Exception as e:\n",
    "                print(f\"Error merging with stock factors: {e}\")\n",
    "                final_data = aligned_data\n",
    "        else:\n",
    "            final_data = aligned_data\n",
    "        \n",
    "        # Handle any remaining missing values\n",
    "        final_data = final_data.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        # Optional: Save to file\n",
    "        output_file = os.path.join(self.cache_dir, 'preprocessed', 'modeling_dataset.csv')\n",
    "        final_data.to_csv(output_file, index=False)\n",
    "        print(f\"Saved modeling dataset to: {output_file}\")\n",
    "        \n",
    "        return final_data\n",
    "\n",
    "\n",
    "def run_data_collection():\n",
    "    \"\"\"\n",
    "    Run the complete data collection process\n",
    "    \"\"\"\n",
    "    # Initialize data collector\n",
    "    collector = EquityDataCollector(cache_dir='data')\n",
    "    \n",
    "    # Step 1: Get S&P 500 constituents and sample stocks\n",
    "    sp500 = collector.get_sp500_constituents(refresh=True)\n",
    "    sampled_stocks = collector.get_sample_stocks(n_stocks=80, min_per_sector=5)\n",
    "    \n",
    "    # Get list of tickers\n",
    "    tickers = sampled_stocks['ticker'].tolist()\n",
    "    \n",
    "    # For debugging/testing with fewer stocks\n",
    "    # tickers = tickers[:10]  # Uncomment to use fewer stocks\n",
    "    \n",
    "    # Add SPY for market benchmark\n",
    "    if 'SPY' not in tickers:\n",
    "        tickers.append('SPY')\n",
    "    \n",
    "    # Step 2: Fetch historical price data\n",
    "    price_data = collector.fetch_historical_prices(\n",
    "        tickers, \n",
    "        start_date='2020-01-01',\n",
    "        end_date=None,  # Use today as end date\n",
    "        interval='1d',\n",
    "        refresh=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Fetch factor data\n",
    "    factor_data = collector.fetch_factor_data(\n",
    "        start_date='2020-01-01',\n",
    "        end_date=None,  # Use today as end date\n",
    "        refresh=True\n",
    "    )\n",
    "    \n",
    "    # Step 4: Get options data for a subset of stocks\n",
    "    # Choose liquid stocks for options analysis\n",
    "    option_tickers = ['SPY', 'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA', 'JPM', 'BAC']\n",
    "    \n",
    "    # Ensure all option_tickers are in the main ticker list\n",
    "    option_tickers = [ticker for ticker in option_tickers if ticker in tickers or ticker == 'SPY']\n",
    "    \n",
    "    # Fetch options data\n",
    "    options_data = collector.get_options_data(\n",
    "        option_tickers,\n",
    "        dte_min=10,\n",
    "        dte_max=120,\n",
    "        refresh=True\n",
    "    )\n",
    "    \n",
    "    # Initialize data preprocessor\n",
    "    preprocessor = DataPreprocessor(cache_dir='data')\n",
    "    \n",
    "    # Step 5: Calculate stock-specific factors\n",
    "    stock_factors = preprocessor.calculate_stock_factors(price_data)\n",
    "    \n",
    "    # Step 6: Prepare modeling dataset\n",
    "    modeling_data = preprocessor.prepare_modeling_dataset(price_data, factor_data, stock_factors)\n",
    "    \n",
    "    # Save the results for easy access\n",
    "    result = {\n",
    "        'price_data': price_data,\n",
    "        'factor_data': factor_data,\n",
    "        'stock_factors': stock_factors,\n",
    "        'options_data': options_data,\n",
    "        'modeling_data': modeling_data\n",
    "    }\n",
    "    \n",
    "    # Save a summary of what data was collected\n",
    "    summary = {\n",
    "        'num_stocks': len(price_data),\n",
    "        'date_range': (modeling_data['date'].min(), modeling_data['date'].max()) if not modeling_data.empty else None,\n",
    "        'num_options_stocks': len(options_data),\n",
    "        'option_tickers': list(options_data.keys()) if options_data else []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nData Collection Summary:\")\n",
    "    print(f\"Collected price data for {summary['num_stocks']} stocks\")\n",
    "    if summary['date_range']:\n",
    "        print(f\"Date range: {summary['date_range'][0]} to {summary['date_range'][1]}\")\n",
    "    print(f\"Collected options data for {summary['num_options_stocks']} stocks\")\n",
    "    print(f\"Options tickers: {', '.join(summary['option_tickers'])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Exploratory data analysis function\n",
    "def explore_data(data):\n",
    "    \"\"\"\n",
    "    Perform basic exploratory data analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Dictionary containing the collected data\n",
    "    \"\"\"\n",
    "    # Create plots directory if it doesn't exist\n",
    "    plots_dir = 'data/plots'\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "    \n",
    "    # 1. Analyze price data\n",
    "    price_data = data['price_data']\n",
    "    \n",
    "    if price_data:\n",
    "        print(\"\\n=== Price Data Analysis ===\")\n",
    "        \n",
    "        # Select a few tickers for visualization\n",
    "        sample_tickers = list(price_data.keys())[:5]\n",
    "        \n",
    "        for ticker in sample_tickers:\n",
    "            prices = price_data[ticker]\n",
    "            \n",
    "            # Plot price chart\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            if 'Close' in prices.columns:\n",
    "                plt.plot(prices.index, prices['Close'])\n",
    "                plt.title(f\"{ticker} Stock Price\")\n",
    "                plt.xlabel(\"Date\")\n",
    "                plt.ylabel(\"Price ($)\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.savefig(f\"{plots_dir}/{ticker}_price.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"Created price chart for {ticker}\")\n",
    "    \n",
    "    # 2. Analyze factor data\n",
    "    factor_data = data['factor_data']\n",
    "    \n",
    "    if not factor_data.empty:\n",
    "        print(\"\\n=== Factor Data Analysis ===\")\n",
    "        \n",
    "        # Plot factor returns\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        factor_cols = ['mkt_rf', 'smb', 'hml', 'momentum', 'volatility']\n",
    "        factor_cols = [col for col in factor_cols if col in factor_data.columns]\n",
    "        \n",
    "        if factor_cols:\n",
    "            for col in factor_cols:\n",
    "                cumulative_return = (1 + factor_data[col]).cumprod() - 1\n",
    "                plt.plot(factor_data.index, cumulative_return, label=col)\n",
    "            \n",
    "            plt.title(\"Cumulative Factor Returns\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Cumulative Return\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(f\"{plots_dir}/factor_returns.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"Created factor returns chart\")\n",
    "    \n",
    "    # 3. Analyze options data\n",
    "    options_data = data['options_data']\n",
    "    \n",
    "    if options_data:\n",
    "        print(\"\\n=== Options Data Analysis ===\")\n",
    "        \n",
    "        # Select one ticker for visualization\n",
    "        if 'SPY' in options_data:\n",
    "            sample_ticker = 'SPY'\n",
    "        else:\n",
    "            sample_ticker = list(options_data.keys())[0]\n",
    "        \n",
    "        option_data = options_data[sample_ticker]\n",
    "        \n",
    "        # Extract calls and puts\n",
    "        calls = option_data['calls']\n",
    "        puts = option_data['puts']\n",
    "        \n",
    "        # Print basic stats\n",
    "        print(f\"{sample_ticker} options data:\")\n",
    "        print(f\"  Spot price: ${option_data['spot_price']:.2f}\")\n",
    "        print(f\"  Call options: {len(calls)}\")\n",
    "        print(f\"  Put options: {len(puts)}\")\n",
    "        \n",
    "        # Plot implied volatility smile for one expiration\n",
    "        if 'expirationDate' in calls.columns and 'moneyness' in calls.columns and 'impliedVolatility' in calls.columns:\n",
    "            expirations = sorted(calls['expirationDate'].unique())\n",
    "            \n",
    "            if expirations:\n",
    "                # Get nearest expiration\n",
    "                nearest_exp = expirations[0]\n",
    "                \n",
    "                # Filter calls and puts for this expiration\n",
    "                exp_calls = calls[calls['expirationDate'] == nearest_exp]\n",
    "                exp_puts = puts[puts['expirationDate'] == nearest_exp]\n",
    "                \n",
    "                # Plot IV smile\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                if not exp_calls.empty:\n",
    "                    plt.scatter(exp_calls['moneyness'], exp_calls['impliedVolatility'], \n",
    "                              label='Calls', alpha=0.7, color='blue')\n",
    "                \n",
    "                if not exp_puts.empty:\n",
    "                    plt.scatter(exp_puts['moneyness'], exp_puts['impliedVolatility'], \n",
    "                              label='Puts', alpha=0.7, color='red')\n",
    "                \n",
    "                plt.axvline(x=1.0, color='black', linestyle='--', alpha=0.5, label='ATM')\n",
    "                \n",
    "                plt.title(f\"{sample_ticker} Implied Volatility Smile - {nearest_exp}\")\n",
    "                plt.xlabel(\"Moneyness (Strike/Spot)\")\n",
    "                plt.ylabel(\"Implied Volatility\")\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.savefig(f\"{plots_dir}/{sample_ticker}_iv_smile.png\")\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"Created IV smile chart for {sample_ticker}\")\n",
    "    \n",
    "    # 4. Analyze modeling dataset\n",
    "    modeling_data = data['modeling_data']\n",
    "    \n",
    "    if not modeling_data.empty:\n",
    "        print(\"\\n=== Modeling Data Analysis ===\")\n",
    "        \n",
    "        # Print basic stats\n",
    "        print(f\"Modeling dataset shape: {modeling_data.shape}\")\n",
    "        print(f\"Number of tickers: {modeling_data['ticker'].nunique()}\")\n",
    "        \n",
    "        # Correlation of factors and returns\n",
    "        numeric_cols = ['return', 'mkt_rf', 'smb', 'hml', 'momentum', 'volatility']\n",
    "        numeric_cols = [col for col in numeric_cols if col in modeling_data.columns]\n",
    "        \n",
    "        if numeric_cols:\n",
    "            corr = modeling_data[numeric_cols].corr()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "            plt.title(\"Factor Correlation Matrix\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plots_dir}/factor_correlation.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            print(\"Created factor correlation matrix\")\n",
    "    \n",
    "    print(\"\\nExploratory data analysis complete. Charts saved to data/plots/ directory.\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run data collection\n",
    "    collected_data = run_data_collection()\n",
    "    \n",
    "    # Run exploratory data analysis\n",
    "    explore_data(collected_data)\n",
    "    \n",
    "    print(\"\\nPhase 1 (Data Collection and Exploration) complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
